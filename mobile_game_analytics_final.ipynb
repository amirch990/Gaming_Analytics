{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mobile Game Analytics: Player Behavior & Monetization Deep Dive\n",
        "\n",
        "**Author:** Amir  \n",
        "**Environment:** Google BigQuery + Python (GCP Colab)  \n",
        "**Dataset:** 15M+ player events from a mobile game (Dec 2021 - Mar 2022)\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This analysis examines player behavior, retention, and monetization patterns in a mobile game with approximately 26,000 users and 15 million event records. The key business questions addressed are:\n",
        "\n",
        "1. **How well does the game retain players?** (Day 1, Day 7, Day 30 retention)\n",
        "2. **What drives monetization?** (Conversion rates, windowed LTV, time-to-purchase)\n",
        "3. **Where do players churn vs. convert?** (Level-based analysis)\n",
        "4. **Are there significant differences between cohorts?** (Statistical validation with effect sizes)\n",
        "5. **Can we segment players by behavior?** (Rule-based archetypes & conversion strategy)\n",
        "\n",
        "### Key Findings (Preview)\n",
        "- **Retention:** Day 1 retention averages ~35%, with significant variance across cohorts\n",
        "- **Monetization:** 81% of first purchases occur at Level 1, with 1.68% overall payer conversion\n",
        "- **Churn Hotspot:** Level 6 shows the highest churn concentration (23.5% of all churned users)\n",
        "- **Statistical Insight:** January cohort shows higher LTV30 than February (with effect size and CI)\n",
        "- **Segmentation:** Grinders show high engagement but zero spend â€” key conversion opportunity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Environment Setup & Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistical testing\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, mannwhitneyu, kruskal\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Table reference\n",
        "TABLE = 'gaming-analytics.mobile_game.events'\n",
        "\n",
        "print(\"Environment ready.\")\n",
        "print(f\"Target table: {TABLE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Bootstrap confidence interval\n",
        "def bootstrap_ci(data, statistic=np.mean, n_bootstrap=1000, ci=0.95):\n",
        "    \"\"\"\n",
        "    Calculate bootstrap confidence interval for a statistic.\n",
        "    Returns: (point_estimate, ci_lower, ci_upper)\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    boot_stats = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=len(data), replace=True)\n",
        "        boot_stats.append(statistic(sample))\n",
        "    \n",
        "    point_est = statistic(data)\n",
        "    alpha = 1 - ci\n",
        "    ci_lower = np.percentile(boot_stats, alpha/2 * 100)\n",
        "    ci_upper = np.percentile(boot_stats, (1 - alpha/2) * 100)\n",
        "    \n",
        "    return point_est, ci_lower, ci_upper\n",
        "\n",
        "print(\"Helper functions loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Preview\n",
        "\n",
        "Let's first examine the raw data structure and get a feel for what we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the data\n",
        "query_preview = f\"\"\"\n",
        "SELECT *\n",
        "FROM `{TABLE}`\n",
        "LIMIT 10000\n",
        "\"\"\"\n",
        "\n",
        "df_preview = client.query(query_preview).to_dataframe()\n",
        "\n",
        "print(\"DATA PREVIEW\")\n",
        "print(\"=\"*60)\n",
        "display(df_preview.head(10))\n",
        "\n",
        "print(\"\\nDATA INFO\")\n",
        "print(\"=\"*60)\n",
        "print(df_preview.info())\n",
        "\n",
        "print(\"\\nSAMPLE SHAPE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Preview sample: {df_preview.shape[0]:,} rows Ã— {df_preview.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Data Schema & Coverage\n",
        "\n",
        "Now let's understand the full dataset: event types, date range, and scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data overview query\n",
        "query_overview = f\"\"\"\n",
        "SELECT \n",
        "    COUNT(1) as total_events,\n",
        "    COUNT(DISTINCT uid) as unique_users,\n",
        "    MIN(DATE(event_time)) as first_date,\n",
        "    MAX(DATE(event_time)) as last_date,\n",
        "    COUNT(DISTINCT event) as event_types,\n",
        "    COUNT(DISTINCT DATE(event_time)) as days_of_data\n",
        "FROM `{TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "df_overview = client.query(query_overview).to_dataframe()\n",
        "print(\"=\" * 50)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total Events:     {df_overview['total_events'].iloc[0]:,}\")\n",
        "print(f\"Unique Users:     {df_overview['unique_users'].iloc[0]:,}\")\n",
        "print(f\"Date Range:       {df_overview['first_date'].iloc[0]} to {df_overview['last_date'].iloc[0]}\")\n",
        "print(f\"Days of Data:     {df_overview['days_of_data'].iloc[0]}\")\n",
        "print(f\"Event Types:      {df_overview['event_types'].iloc[0]}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Event types breakdown\n",
        "query_events = f\"\"\"\n",
        "SELECT \n",
        "    event,\n",
        "    COUNT(1) as count,\n",
        "    ROUND(COUNT(1) * 100.0 / SUM(COUNT(1)) OVER(), 2) as pct\n",
        "FROM `{TABLE}`\n",
        "GROUP BY 1\n",
        "ORDER BY 2 DESC\n",
        "\"\"\"\n",
        "\n",
        "df_events = client.query(query_events).to_dataframe()\n",
        "df_events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation:** The dataset captures the full player journey - from sessions and gameplay (`Play`, `Session_Start/End`) to social features (`Attack`, `Steal`, `Invite_Friend`) and monetization (`Store_Open/Close`, `InApp_Purchase`). The `Play` event dominates at ~52% of all events, which is expected for an action-oriented game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Data Quality Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality checks\n",
        "query_quality = f\"\"\"\n",
        "SELECT\n",
        "    -- Null checks\n",
        "    COUNTIF(uid IS NULL) as null_uid,\n",
        "    COUNTIF(event IS NULL) as null_event,\n",
        "    COUNTIF(event_time IS NULL) as null_event_time,\n",
        "    \n",
        "    -- Event pair validation (Start/End matching)\n",
        "    COUNTIF(event = 'Session_Start') as session_starts,\n",
        "    COUNTIF(event = 'Session_End') as session_ends,\n",
        "    COUNTIF(event = 'Store_Open') as store_opens,\n",
        "    COUNTIF(event = 'Store_Close') as store_closes,\n",
        "    \n",
        "    -- Purchase validation\n",
        "    COUNTIF(event = 'InApp_Purchase') as purchases,\n",
        "    COUNTIF(event = 'InApp_Purchase_Canceled') as canceled\n",
        "    \n",
        "FROM `{TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "df_quality = client.query(query_quality).to_dataframe()\n",
        "\n",
        "print(\"DATA QUALITY REPORT\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Null UIDs:          {df_quality['null_uid'].iloc[0]}\")\n",
        "print(f\"Null Events:        {df_quality['null_event'].iloc[0]}\")\n",
        "print(f\"Null Timestamps:    {df_quality['null_event_time'].iloc[0]}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Session Start/End:  {df_quality['session_starts'].iloc[0]:,} / {df_quality['session_ends'].iloc[0]:,}\")\n",
        "print(f\"Store Open/Close:   {df_quality['store_opens'].iloc[0]:,} / {df_quality['store_closes'].iloc[0]:,}\")\n",
        "print(f\"Purchases/Canceled: {df_quality['purchases'].iloc[0]:,} / {df_quality['canceled'].iloc[0]:,}\")\n",
        "print(\"-\" * 40)\n",
        "print(\"âœ“ Data quality is suitable for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: User Acquisition & Engagement\n",
        "\n",
        "Before diving into retention and monetization, let's understand the user base: when did users join, and how engaged are they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Cohort Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install cohorts\n",
        "query_installs = f\"\"\"\n",
        "WITH user_install AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as install_date,\n",
        "        DATE_TRUNC(DATE(MIN(event_time)), MONTH) as cohort_month\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    cohort_month,\n",
        "    COUNT(DISTINCT uid) as users,\n",
        "    MIN(install_date) as first_install,\n",
        "    MAX(install_date) as last_install\n",
        "FROM user_install\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_cohorts = client.query(query_installs).to_dataframe()\n",
        "df_cohorts['pct'] = (df_cohorts['users'] / df_cohorts['users'].sum() * 100).round(1)\n",
        "\n",
        "print(\"COHORT SIZES\")\n",
        "print(df_cohorts.to_string(index=False))\n",
        "print(f\"\\nTotal Users: {df_cohorts['users'].sum():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Daily Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily installs - FIXED SQL\n",
        "query_daily_installs = f\"\"\"\n",
        "WITH user_installs AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as install_date\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY uid\n",
        ")\n",
        "SELECT \n",
        "    install_date,\n",
        "    COUNT(*) as installs\n",
        "FROM user_installs\n",
        "WHERE install_date >= '2021-12-30'\n",
        "GROUP BY install_date\n",
        "ORDER BY install_date\n",
        "\"\"\"\n",
        "\n",
        "df_daily = client.query(query_daily_installs).to_dataframe()\n",
        "df_daily['install_date'] = pd.to_datetime(df_daily['install_date'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(df_daily['install_date'], df_daily['installs'], color='#4285F4', linewidth=1.5)\n",
        "ax.fill_between(df_daily['install_date'], df_daily['installs'], alpha=0.3, color='#4285F4')\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Daily Installs')\n",
        "ax.set_title('Daily New User Installs', fontsize=14, fontweight='bold')\n",
        "ax.axhline(y=df_daily['installs'].mean(), color='red', linestyle='--', alpha=0.7, label=f\"Avg: {df_daily['installs'].mean():.0f}\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average daily installs: {df_daily['installs'].mean():.0f}\")\n",
        "print(f\"Peak day: {df_daily.loc[df_daily['installs'].idxmax(), 'install_date'].strftime('%Y-%m-%d')} ({df_daily['installs'].max()} installs)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation:** January 2022 represents the largest cohort (52% of users), likely indicating a user acquisition campaign or feature launch. The March cohort is notably smaller and incomplete, which we'll account for in retention calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Engagement: DAU and Stickiness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DAU and MAU for stickiness calculation\n",
        "query_engagement = f\"\"\"\n",
        "WITH daily_users AS (\n",
        "    SELECT \n",
        "        DATE(event_time) as dt,\n",
        "        COUNT(DISTINCT uid) as dau\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "monthly_users AS (\n",
        "    SELECT \n",
        "        DATE_TRUNC(DATE(event_time), MONTH) as month,\n",
        "        COUNT(DISTINCT uid) as mau\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    d.dt,\n",
        "    d.dau,\n",
        "    m.mau,\n",
        "    ROUND(d.dau * 100.0 / m.mau, 2) as stickiness\n",
        "FROM daily_users d\n",
        "JOIN monthly_users m ON DATE_TRUNC(d.dt, MONTH) = m.month\n",
        "WHERE d.dt >= '2022-01-01'\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_engagement = client.query(query_engagement).to_dataframe()\n",
        "df_engagement['dt'] = pd.to_datetime(df_engagement['dt'])\n",
        "\n",
        "# Plot DAU with stickiness\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "# DAU\n",
        "ax1.plot(df_engagement['dt'], df_engagement['dau'], color='#4285F4', linewidth=1.5)\n",
        "ax1.set_ylabel('DAU')\n",
        "ax1.set_title('Daily Active Users (DAU)', fontsize=12, fontweight='bold')\n",
        "ax1.axhline(y=df_engagement['dau'].mean(), color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Stickiness (DAU/MAU)\n",
        "ax2.plot(df_engagement['dt'], df_engagement['stickiness'], color='#34A853', linewidth=1.5)\n",
        "ax2.set_ylabel('Stickiness (%)')\n",
        "ax2.set_xlabel('Date')\n",
        "ax2.set_title('Stickiness (DAU/MAU Ratio)', fontsize=12, fontweight='bold')\n",
        "ax2.axhline(y=df_engagement['stickiness'].mean(), color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage DAU: {df_engagement['dau'].mean():.0f}\")\n",
        "print(f\"Average Stickiness (DAU/MAU): {df_engagement['stickiness'].mean():.1f}%\")\n",
        "print(f\"\\nInterpretation: On average, {df_engagement['stickiness'].mean():.1f}% of monthly users play on any given day.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:** A stickiness ratio of ~5-10% is typical for casual mobile games. Higher would indicate a highly engaging core loop. The declining trend suggests natural attrition as cohorts age without sufficient new user acquisition to offset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Retention Analysis\n",
        "\n",
        "Retention is the most critical metric for any mobile game. We'll calculate classic Day 1, Day 7, and Day 30 retention rates by cohort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Classic Retention Curves (D1, D7, D30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate retention for D1, D7, D14, D30\n",
        "query_retention = f\"\"\"\n",
        "WITH user_install AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as install_date,\n",
        "        DATE_TRUNC(DATE(MIN(event_time)), MONTH) as cohort_month\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "user_activity AS (\n",
        "    SELECT DISTINCT\n",
        "        uid,\n",
        "        DATE(event_time) as activity_date\n",
        "    FROM `{TABLE}`\n",
        "),\n",
        "\n",
        "retention_base AS (\n",
        "    SELECT \n",
        "        i.cohort_month,\n",
        "        i.uid,\n",
        "        i.install_date,\n",
        "        DATE_DIFF(a.activity_date, i.install_date, DAY) as day_n\n",
        "    FROM user_install i\n",
        "    LEFT JOIN user_activity a ON i.uid = a.uid\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    cohort_month,\n",
        "    COUNT(DISTINCT uid) as cohort_size,\n",
        "    COUNT(DISTINCT CASE WHEN day_n = 1 THEN uid END) as d1_retained,\n",
        "    COUNT(DISTINCT CASE WHEN day_n = 7 THEN uid END) as d7_retained,\n",
        "    COUNT(DISTINCT CASE WHEN day_n = 14 THEN uid END) as d14_retained,\n",
        "    COUNT(DISTINCT CASE WHEN day_n = 30 THEN uid END) as d30_retained,\n",
        "    ROUND(COUNT(DISTINCT CASE WHEN day_n = 1 THEN uid END) * 100.0 / COUNT(DISTINCT uid), 1) as d1_pct,\n",
        "    ROUND(COUNT(DISTINCT CASE WHEN day_n = 7 THEN uid END) * 100.0 / COUNT(DISTINCT uid), 1) as d7_pct,\n",
        "    ROUND(COUNT(DISTINCT CASE WHEN day_n = 14 THEN uid END) * 100.0 / COUNT(DISTINCT uid), 1) as d14_pct,\n",
        "    ROUND(COUNT(DISTINCT CASE WHEN day_n = 30 THEN uid END) * 100.0 / COUNT(DISTINCT uid), 1) as d30_pct\n",
        "FROM retention_base\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_retention = client.query(query_retention).to_dataframe()\n",
        "df_retention['cohort_month'] = pd.to_datetime(df_retention['cohort_month'])\n",
        "df_retention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retention curve visualization\n",
        "retention_days = [0, 1, 7, 14, 30]\n",
        "colors = {'2021-12': '#4285F4', '2022-01': '#EA4335', '2022-02': '#FBBC04', '2022-03': '#34A853'}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "for _, row in df_retention.iterrows():\n",
        "    cohort_label = row['cohort_month'].strftime('%Y-%m')\n",
        "    retention_values = [100, row['d1_pct'], row['d7_pct'], row['d14_pct'], row['d30_pct']]\n",
        "    \n",
        "    # Only plot if we have valid D30 data\n",
        "    if row['d30_pct'] > 0:\n",
        "        ax.plot(retention_days, retention_values, marker='o', linewidth=2, \n",
        "                label=f\"{cohort_label} (n={row['cohort_size']:,})\", color=colors.get(cohort_label, 'gray'))\n",
        "\n",
        "ax.set_xlabel('Days Since Install', fontsize=11)\n",
        "ax.set_ylabel('Retention Rate (%)', fontsize=11)\n",
        "ax.set_title('Retention Curves by Monthly Cohort', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(retention_days)\n",
        "ax.set_xticklabels(['D0', 'D1', 'D7', 'D14', 'D30'])\n",
        "ax.legend(title='Cohort')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim(0, 105)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary retention table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETENTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "summary = df_retention[df_retention['d30_pct'] > 0][['cohort_month', 'cohort_size', 'd1_pct', 'd7_pct', 'd14_pct', 'd30_pct']].copy()\n",
        "summary['cohort_month'] = summary['cohort_month'].dt.strftime('%Y-%m')\n",
        "summary.columns = ['Cohort', 'Users', 'D1 %', 'D7 %', 'D14 %', 'D30 %']\n",
        "print(summary.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Industry benchmarks context\n",
        "print(\"\\nBenchmark Context (Casual Mobile Games):\")\n",
        "print(\"  D1: 25-40% (Good)\")\n",
        "print(\"  D7: 10-15% (Good)\")\n",
        "print(\"  D30: 3-7% (Good)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Statistical Test: D7 Retention Comparison (with Effect Size & CI)\n",
        "\n",
        "Let's test if the observed difference in Day 7 retention between January and February cohorts is statistically significant, **including effect size and confidence intervals**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-square test with effect size and confidence interval\n",
        "# H0: Retention rates are the same across cohorts\n",
        "# H1: Retention rates differ significantly\n",
        "\n",
        "# Get raw counts for January vs February D7 retention\n",
        "jan = df_retention[df_retention['cohort_month'].dt.month == 1].iloc[0]\n",
        "feb = df_retention[df_retention['cohort_month'].dt.month == 2].iloc[0]\n",
        "\n",
        "# Build contingency table: [retained, not_retained] for each cohort\n",
        "jan_retained = int(jan['d7_retained'])\n",
        "jan_total = int(jan['cohort_size'])\n",
        "jan_not_retained = jan_total - jan_retained\n",
        "\n",
        "feb_retained = int(feb['d7_retained'])\n",
        "feb_total = int(feb['cohort_size'])\n",
        "feb_not_retained = feb_total - feb_retained\n",
        "\n",
        "contingency_table = np.array([\n",
        "    [jan_retained, jan_not_retained],\n",
        "    [feb_retained, feb_not_retained]\n",
        "])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Calculate effect size (difference in percentage points)\n",
        "jan_rate = jan_retained / jan_total\n",
        "feb_rate = feb_retained / feb_total\n",
        "effect_size_pp = (jan_rate - feb_rate) * 100  # in percentage points\n",
        "\n",
        "# Calculate 95% CI for the difference using normal approximation\n",
        "se_diff = np.sqrt((jan_rate * (1 - jan_rate) / jan_total) + \n",
        "                  (feb_rate * (1 - feb_rate) / feb_total))\n",
        "ci_lower = (jan_rate - feb_rate - 1.96 * se_diff) * 100\n",
        "ci_upper = (jan_rate - feb_rate + 1.96 * se_diff) * 100\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL TEST: D7 Retention Comparison (Jan vs Feb)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nJanuary Cohort:  {jan['d7_pct']}% D7 retention (n={jan_total:,})\")\n",
        "print(f\"February Cohort: {feb['d7_pct']}% D7 retention (n={feb_total:,})\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"EFFECT SIZE & CONFIDENCE INTERVAL\")\n",
        "print(\"-\"*70)\n",
        "print(f\"  Î” (Jan - Feb):     {effect_size_pp:+.2f} percentage points\")\n",
        "print(f\"  95% CI:            [{ci_lower:.2f}, {ci_upper:.2f}] pp\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STATISTICAL SIGNIFICANCE\")\n",
        "print(\"-\"*70)\n",
        "print(f\"  Chi-square stat:   {chi2:.2f}\")\n",
        "print(f\"  P-value:           {p_value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"âœ“ SIGNIFICANT (p < {alpha})\")\n",
        "    if ci_lower > 0:\n",
        "        print(f\"  January retention is {effect_size_pp:.1f}pp higher (CI excludes 0)\")\n",
        "    elif ci_upper < 0:\n",
        "        print(f\"  February retention is {abs(effect_size_pp):.1f}pp higher (CI excludes 0)\")\n",
        "else:\n",
        "    print(f\"âœ— NOT SIGNIFICANT (p >= {alpha})\")\n",
        "    print(f\"  The observed {abs(effect_size_pp):.1f}pp difference may be due to random variation.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Monetization Analysis\n",
        "\n",
        "Now let's examine how users convert to payers and what drives revenue. \n",
        "\n",
        "**Important methodological note:** When comparing LTV across cohorts with different ages, we must use **windowed LTV** (e.g., LTV7, LTV30) to ensure fair comparison. Otherwise, older cohorts will appear to have higher LTV simply because they had more time to spend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Monetization Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive monetization summary\n",
        "query_monetization = f\"\"\"\n",
        "WITH user_revenue AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        SUM(CASE WHEN event = 'InApp_Purchase' THEN price ELSE 0 END) as lifetime_revenue,\n",
        "        COUNTIF(event = 'InApp_Purchase') as purchase_count\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    COUNT(DISTINCT uid) as total_users,\n",
        "    COUNTIF(lifetime_revenue > 0) as paying_users,\n",
        "    SUM(lifetime_revenue) as total_revenue,\n",
        "    ROUND(SUM(lifetime_revenue) / COUNT(DISTINCT uid), 2) as ltv_all,\n",
        "    ROUND(SUM(lifetime_revenue) / NULLIF(COUNTIF(lifetime_revenue > 0), 0), 2) as arppu,\n",
        "    ROUND(COUNTIF(lifetime_revenue > 0) * 100.0 / COUNT(DISTINCT uid), 2) as conversion_rate,\n",
        "    ROUND(AVG(CASE WHEN lifetime_revenue > 0 THEN purchase_count END), 1) as avg_purchases_per_payer\n",
        "FROM user_revenue\n",
        "\"\"\"\n",
        "\n",
        "df_monetization = client.query(query_monetization).to_dataframe()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MONETIZATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Users:           {df_monetization['total_users'].iloc[0]:,}\")\n",
        "print(f\"Paying Users:          {df_monetization['paying_users'].iloc[0]:,}\")\n",
        "print(f\"Conversion Rate:       {df_monetization['conversion_rate'].iloc[0]}%\")\n",
        "print(\"-\"*60)\n",
        "print(f\"Total Revenue:         ${df_monetization['total_revenue'].iloc[0]:,.0f}\")\n",
        "print(f\"LTV (all users):       ${df_monetization['ltv_all'].iloc[0]:.2f}\")\n",
        "print(f\"ARPPU (payers only):   ${df_monetization['arppu'].iloc[0]:.2f}\")\n",
        "print(f\"Avg Purchases/Payer:   {df_monetization['avg_purchases_per_payer'].iloc[0]}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Windowed LTV by Cohort (Bias-Corrected)\n",
        "\n",
        "To fairly compare cohorts, we calculate **LTV7** (revenue in first 7 days) and **LTV30** (revenue in first 30 days) for each user. This eliminates the age bias that affects raw lifetime revenue comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowed LTV: LTV7 and LTV30 per user\n",
        "query_windowed_ltv = f\"\"\"\n",
        "WITH user_install AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as install_date,\n",
        "        DATE_TRUNC(DATE(MIN(event_time)), MONTH) as cohort_month\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "user_windowed_revenue AS (\n",
        "    SELECT \n",
        "        f.uid,\n",
        "        i.cohort_month,\n",
        "        i.install_date,\n",
        "        -- LTV7: Revenue within first 7 days\n",
        "        SUM(CASE \n",
        "            WHEN f.event = 'InApp_Purchase' \n",
        "             AND DATE_DIFF(DATE(f.event_time), i.install_date, DAY) <= 7 \n",
        "            THEN f.price ELSE 0 \n",
        "        END) as ltv_7,\n",
        "        -- LTV30: Revenue within first 30 days\n",
        "        SUM(CASE \n",
        "            WHEN f.event = 'InApp_Purchase' \n",
        "             AND DATE_DIFF(DATE(f.event_time), i.install_date, DAY) <= 30 \n",
        "            THEN f.price ELSE 0 \n",
        "        END) as ltv_30,\n",
        "        -- Total LTV (for reference)\n",
        "        SUM(CASE WHEN f.event = 'InApp_Purchase' THEN f.price ELSE 0 END) as ltv_total\n",
        "    FROM `{TABLE}` f\n",
        "    JOIN user_install i ON f.uid = i.uid\n",
        "    GROUP BY 1, 2, 3\n",
        ")\n",
        "\n",
        "SELECT * FROM user_windowed_revenue\n",
        "\"\"\"\n",
        "\n",
        "df_windowed_ltv = client.query(query_windowed_ltv).to_dataframe()\n",
        "df_windowed_ltv['cohort_month'] = pd.to_datetime(df_windowed_ltv['cohort_month'])\n",
        "\n",
        "print(f\"Loaded windowed LTV for {len(df_windowed_ltv):,} users\")\n",
        "df_windowed_ltv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate windowed LTV by cohort\n",
        "ltv_by_cohort = df_windowed_ltv.groupby('cohort_month').agg({\n",
        "    'uid': 'count',\n",
        "    'ltv_7': ['mean', 'sum'],\n",
        "    'ltv_30': ['mean', 'sum'],\n",
        "    'ltv_total': ['mean', 'sum']\n",
        "}).round(2)\n",
        "\n",
        "ltv_by_cohort.columns = ['Users', 'LTV7_Avg', 'LTV7_Total', 'LTV30_Avg', 'LTV30_Total', 'LTV_Total_Avg', 'LTV_Total_Sum']\n",
        "ltv_by_cohort = ltv_by_cohort.reset_index()\n",
        "ltv_by_cohort['cohort_month'] = ltv_by_cohort['cohort_month'].dt.strftime('%Y-%m')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"WINDOWED LTV BY COHORT (Bias-Corrected)\")\n",
        "print(\"=\"*80)\n",
        "print(ltv_by_cohort[['cohort_month', 'Users', 'LTV7_Avg', 'LTV30_Avg', 'LTV_Total_Avg']].to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "print(\"\\nâš ï¸  Note: Use LTV7 or LTV30 for cohort comparisons, not Total LTV\")\n",
        "print(\"   Total LTV is biased by cohort age (older cohorts had more time to spend)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize windowed LTV comparison\n",
        "cohorts = ['2021-12', '2022-01', '2022-02']\n",
        "ltv_plot = ltv_by_cohort[ltv_by_cohort['cohort_month'].isin(cohorts)].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "x = np.arange(len(ltv_plot))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, ltv_plot['LTV7_Avg'], width, label='LTV7', color='#4285F4')\n",
        "bars2 = ax.bar(x, ltv_plot['LTV30_Avg'], width, label='LTV30', color='#34A853')\n",
        "bars3 = ax.bar(x + width, ltv_plot['LTV_Total_Avg'], width, label='Total LTV (biased)', color='#EA4335', alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Cohort')\n",
        "ax.set_ylabel('Average LTV ($)')\n",
        "ax.set_title('Windowed LTV by Cohort (Apples-to-Apples Comparison)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(ltv_plot['cohort_month'])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'${height:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Statistical Test: LTV30 Comparison (with Effect Size & CI)\n",
        "\n",
        "Now we can fairly compare January vs February cohorts using **LTV30** (same observation window)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical test on LTV30 (bias-corrected)\n",
        "jan_ltv30 = df_windowed_ltv[df_windowed_ltv['cohort_month'].dt.month == 1]['ltv_30']\n",
        "feb_ltv30 = df_windowed_ltv[df_windowed_ltv['cohort_month'].dt.month == 2]['ltv_30']\n",
        "\n",
        "# Mann-Whitney U test (non-parametric, handles skewed revenue distributions)\n",
        "stat, p_value = mannwhitneyu(jan_ltv30, feb_ltv30, alternative='two-sided')\n",
        "\n",
        "# Effect size: difference in means (in dollars)\n",
        "jan_mean = jan_ltv30.mean()\n",
        "feb_mean = feb_ltv30.mean()\n",
        "effect_size_dollars = jan_mean - feb_mean\n",
        "\n",
        "# Bootstrap 95% CI for the difference in means\n",
        "np.random.seed(42)\n",
        "n_bootstrap = 5000\n",
        "boot_diffs = []\n",
        "for _ in range(n_bootstrap):\n",
        "    jan_sample = np.random.choice(jan_ltv30, size=len(jan_ltv30), replace=True)\n",
        "    feb_sample = np.random.choice(feb_ltv30, size=len(feb_ltv30), replace=True)\n",
        "    boot_diffs.append(jan_sample.mean() - feb_sample.mean())\n",
        "\n",
        "ci_lower = np.percentile(boot_diffs, 2.5)\n",
        "ci_upper = np.percentile(boot_diffs, 97.5)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL TEST: LTV30 Comparison (Jan vs Feb)\")\n",
        "print(\"Using windowed LTV to eliminate cohort age bias\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nJanuary Cohort:  ${jan_mean:.3f} avg LTV30 (n={len(jan_ltv30):,})\")\n",
        "print(f\"February Cohort: ${feb_mean:.3f} avg LTV30 (n={len(feb_ltv30):,})\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"EFFECT SIZE & CONFIDENCE INTERVAL\")\n",
        "print(\"-\"*70)\n",
        "print(f\"  Î” (Jan - Feb):     ${effect_size_dollars:+.3f}\")\n",
        "print(f\"  95% CI (bootstrap): [${ci_lower:.3f}, ${ci_upper:.3f}]\")\n",
        "if jan_mean > 0 and feb_mean > 0:\n",
        "    pct_diff = (jan_mean / feb_mean - 1) * 100\n",
        "    print(f\"  Relative diff:     {pct_diff:+.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STATISTICAL SIGNIFICANCE\")\n",
        "print(\"-\"*70)\n",
        "print(f\"  Mann-Whitney U:    {stat:,.0f}\")\n",
        "print(f\"  P-value:           {p_value:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"âœ“ SIGNIFICANT (p < {alpha})\")\n",
        "    if ci_lower > 0:\n",
        "        print(f\"  January LTV30 is significantly higher by ${effect_size_dollars:.3f}\")\n",
        "        print(f\"  CI excludes 0 â†’ effect is robust\")\n",
        "    elif ci_upper < 0:\n",
        "        print(f\"  February LTV30 is significantly higher by ${abs(effect_size_dollars):.3f}\")\n",
        "    else:\n",
        "        print(f\"  Statistically significant but CI includes 0 â†’ interpret with caution\")\n",
        "else:\n",
        "    print(f\"âœ— NOT SIGNIFICANT (p >= {alpha})\")\n",
        "    print(f\"  The ${abs(effect_size_dollars):.3f} difference is not statistically significant.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Time to First Purchase (Conversion Velocity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cumulative conversion by day\n",
        "query_conversion_curve = f\"\"\"\n",
        "WITH user_install AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as install_date,\n",
        "        DATE_TRUNC(DATE(MIN(event_time)), MONTH) as cohort_month\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "first_purchase AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE(MIN(event_time)) as first_purchase_date\n",
        "    FROM `{TABLE}`\n",
        "    WHERE event = 'InApp_Purchase'\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "conversion_by_day AS (\n",
        "    SELECT \n",
        "        i.cohort_month,\n",
        "        DATE_DIFF(p.first_purchase_date, i.install_date, DAY) as days_to_convert,\n",
        "        COUNT(DISTINCT p.uid) as converters\n",
        "    FROM user_install i\n",
        "    INNER JOIN first_purchase p ON i.uid = p.uid\n",
        "    GROUP BY 1, 2\n",
        "),\n",
        "\n",
        "cohort_sizes AS (\n",
        "    SELECT cohort_month, COUNT(DISTINCT uid) as cohort_size\n",
        "    FROM user_install\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    c.cohort_month,\n",
        "    c.days_to_convert,\n",
        "    s.cohort_size,\n",
        "    c.converters,\n",
        "    SUM(c.converters) OVER (PARTITION BY c.cohort_month ORDER BY c.days_to_convert) as cumulative_converters,\n",
        "    ROUND(SUM(c.converters) OVER (PARTITION BY c.cohort_month ORDER BY c.days_to_convert) * 100.0 / s.cohort_size, 3) as cumulative_conversion_pct\n",
        "FROM conversion_by_day c\n",
        "JOIN cohort_sizes s ON c.cohort_month = s.cohort_month\n",
        "WHERE c.days_to_convert <= 90\n",
        "ORDER BY c.cohort_month, c.days_to_convert\n",
        "\"\"\"\n",
        "\n",
        "df_conversion = client.query(query_conversion_curve).to_dataframe()\n",
        "df_conversion['cohort_month'] = pd.to_datetime(df_conversion['cohort_month'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cumulative conversion curves\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "colors = {'2021-12': '#4285F4', '2022-01': '#EA4335', '2022-02': '#FBBC04', '2022-03': '#34A853'}\n",
        "\n",
        "for cohort, group in df_conversion.groupby('cohort_month'):\n",
        "    cohort_label = cohort.strftime('%Y-%m')\n",
        "    cohort_size = group['cohort_size'].iloc[0]\n",
        "    ax.plot(group['days_to_convert'], group['cumulative_conversion_pct'], \n",
        "            linewidth=2, label=f\"{cohort_label} (n={cohort_size:,})\",\n",
        "            color=colors.get(cohort_label, 'gray'))\n",
        "\n",
        "ax.set_xlabel('Days Since Install', fontsize=11)\n",
        "ax.set_ylabel('Cumulative Conversion %', fontsize=11)\n",
        "ax.set_title('Payer Conversion Velocity by Cohort', fontsize=14, fontweight='bold')\n",
        "ax.legend(title='Cohort')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(0, 60)\n",
        "\n",
        "# Add annotation for key insight\n",
        "ax.axvline(x=7, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.text(8, ax.get_ylim()[1]*0.9, 'Day 7', fontsize=9, color='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate % of conversions in first 7 days\n",
        "d7_conv = df_conversion[df_conversion['days_to_convert'] <= 7].groupby('cohort_month')['converters'].sum()\n",
        "total_conv = df_conversion.groupby('cohort_month')['converters'].sum()\n",
        "early_pct = (d7_conv / total_conv * 100).mean()\n",
        "print(f\"\\nInsight: On average, {early_pct:.0f}% of conversions happen within the first 7 days.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Revenue Concentration (Whale Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whale analysis - revenue concentration\n",
        "query_whales = f\"\"\"\n",
        "WITH user_revenue AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        SUM(CASE WHEN event = 'InApp_Purchase' THEN price ELSE 0 END) as lifetime_revenue\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "    HAVING lifetime_revenue > 0\n",
        "),\n",
        "\n",
        "ranked AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        lifetime_revenue,\n",
        "        NTILE(10) OVER (ORDER BY lifetime_revenue DESC) as decile\n",
        "    FROM user_revenue\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    decile,\n",
        "    COUNT(*) as users,\n",
        "    SUM(lifetime_revenue) as revenue,\n",
        "    MIN(lifetime_revenue) as min_rev,\n",
        "    MAX(lifetime_revenue) as max_rev,\n",
        "    ROUND(AVG(lifetime_revenue), 2) as avg_rev\n",
        "FROM ranked\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_whales = client.query(query_whales).to_dataframe()\n",
        "total_rev = df_whales['revenue'].sum()\n",
        "df_whales['pct_revenue'] = (df_whales['revenue'] / total_rev * 100).round(1)\n",
        "df_whales['cumulative_pct'] = df_whales['pct_revenue'].cumsum()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"REVENUE CONCENTRATION BY PAYER DECILE\")\n",
        "print(\"=\"*70)\n",
        "print(\"Decile 1 = Top 10% of payers, Decile 10 = Bottom 10%\")\n",
        "print(\"-\"*70)\n",
        "print(df_whales[['decile', 'users', 'revenue', 'avg_rev', 'pct_revenue', 'cumulative_pct']].to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Key insight\n",
        "top_10_pct = df_whales[df_whales['decile'] == 1]['pct_revenue'].iloc[0]\n",
        "top_20_pct = df_whales[df_whales['decile'] <= 2]['pct_revenue'].sum()\n",
        "print(f\"\\nðŸ‹ Whale Insight:\")\n",
        "print(f\"   Top 10% of payers generate {top_10_pct:.1f}% of revenue\")\n",
        "print(f\"   Top 20% of payers generate {top_20_pct:.1f}% of revenue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Churn & Lifecycle Analysis\n",
        "\n",
        "Understanding where players drop off is crucial for identifying improvement opportunities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Player Lifespan Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Player lifespan\n",
        "query_lifespan = f\"\"\"\n",
        "WITH user_lifespan AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        DATE_DIFF(DATE(MAX(event_time)), DATE(MIN(event_time)), DAY) as lifespan_days\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    CASE \n",
        "        WHEN lifespan_days = 0 THEN '1. Same day'\n",
        "        WHEN lifespan_days BETWEEN 1 AND 7 THEN '2. 1-7 days'\n",
        "        WHEN lifespan_days BETWEEN 8 AND 30 THEN '3. 8-30 days'\n",
        "        WHEN lifespan_days BETWEEN 31 AND 90 THEN '4. 31-90 days'\n",
        "        ELSE '5. 90+ days'\n",
        "    END as lifespan_bucket,\n",
        "    COUNT(*) as users,\n",
        "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as pct\n",
        "FROM user_lifespan\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_lifespan = client.query(query_lifespan).to_dataframe()\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "colors = ['#EA4335', '#FBBC04', '#34A853', '#4285F4', '#9AA0A6']\n",
        "bars = ax.barh(df_lifespan['lifespan_bucket'], df_lifespan['pct'], color=colors)\n",
        "\n",
        "# Add value labels\n",
        "for bar, pct in zip(bars, df_lifespan['pct']):\n",
        "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
        "            f'{pct}%', va='center', fontsize=10)\n",
        "\n",
        "ax.set_xlabel('% of Users')\n",
        "ax.set_title('Player Lifespan Distribution', fontsize=14, fontweight='bold')\n",
        "ax.set_xlim(0, max(df_lifespan['pct']) * 1.15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "same_day = df_lifespan[df_lifespan['lifespan_bucket'].str.contains('Same')]['pct'].iloc[0]\n",
        "print(f\"\\nâš ï¸ {same_day}% of users churn on day 0 (never return after first session)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Critical Insight: Churn vs. First Purchase by Level\n",
        "\n",
        "This analysis reveals the relationship between where users drop off and where they first monetize - a key insight for game design and monetization strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Churn vs First Purchase by Level\n",
        "query_level_analysis = f\"\"\"\n",
        "WITH user_max_level AS (\n",
        "    SELECT uid, MAX(Level) as max_level\n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "first_purchase AS (\n",
        "    SELECT uid, MIN(Level) as first_purchase_level\n",
        "    FROM `{TABLE}`\n",
        "    WHERE event = 'InApp_Purchase'\n",
        "    GROUP BY 1\n",
        ")\n",
        "\n",
        "SELECT \n",
        "    l.max_level as level,\n",
        "    COUNT(DISTINCT l.uid) as churned_at_level,\n",
        "    ROUND(COUNT(DISTINCT l.uid) * 100.0 / SUM(COUNT(DISTINCT l.uid)) OVER(), 1) as pct_churned,\n",
        "    COUNTIF(p.first_purchase_level = l.max_level) as first_purchased_at_level,\n",
        "    ROUND(COUNTIF(p.first_purchase_level = l.max_level) * 100.0 / \n",
        "          NULLIF(SUM(COUNTIF(p.first_purchase_level = l.max_level)) OVER(), 0), 1) as pct_first_purchase\n",
        "FROM user_max_level l\n",
        "LEFT JOIN first_purchase p ON l.uid = p.uid\n",
        "WHERE l.max_level <= 10\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        "\"\"\"\n",
        "\n",
        "df_level = client.query(query_level_analysis).to_dataframe()\n",
        "df_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Dual bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(df_level))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, df_level['pct_churned'], width, label='% Churned at Level', color='#EA4335', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, df_level['pct_first_purchase'].fillna(0), width, label='% First Purchase at Level', color='#34A853', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Player Level', fontsize=11)\n",
        "ax.set_ylabel('Percentage', fontsize=11)\n",
        "ax.set_title('Where Do Players Churn vs. First Purchase?', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_level['level'])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Key insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "level_1_purchase = df_level[df_level['level'] == 1]['pct_first_purchase'].iloc[0]\n",
        "level_6_churn = df_level[df_level['level'] == 6]['pct_churned'].iloc[0]\n",
        "print(f\"\\n1. MONETIZATION WINDOW: {level_1_purchase}% of first purchases occur at Level 1\")\n",
        "print(f\"   â†’ Early monetization triggers are critical\")\n",
        "print(f\"\\n2. CHURN HOTSPOT: Level 6 accounts for {level_6_churn}% of all churn\")\n",
        "print(f\"   â†’ Investigate difficulty spike or content gap at Level 6\")\n",
        "print(f\"\\n3. IMPLICATION: Users who reach Level 2+ rarely convert\")\n",
        "print(f\"   â†’ Consider moving monetization offers earlier in the funnel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Rule-Based Player Segmentation\n",
        "\n",
        "Beyond cohort analysis, we can segment players by their behavioral patterns to identify high-value opportunities.\n",
        "\n",
        "### Methodology Note\n",
        "\n",
        "This section uses **rule-based segmentation** (not unsupervised clustering). We define player archetypes using explicit business rules based on percentile thresholds.\n",
        "\n",
        "**Why rule-based over clustering?**\n",
        "1. **Interpretability**: Stakeholders can understand \"top 1% spenders\" immediately\n",
        "2. **Actionability**: Rules map directly to targeting criteria in marketing tools\n",
        "3. **Stability**: No risk of cluster drift between model runs\n",
        "4. **Business alignment**: Categories match how teams already think about players"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Feature Engineering: Building User Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQL Feature Engineering: User Summary with Player Archetypes\n",
        "query_user_summary = f\"\"\"\n",
        "WITH user_metrics AS (\n",
        "    SELECT \n",
        "        uid,\n",
        "        -- Spending metrics\n",
        "        SUM(CASE WHEN event = 'InApp_Purchase' THEN price ELSE 0 END) as total_spend,\n",
        "        \n",
        "        -- Engagement metrics\n",
        "        COUNTIF(event = 'Session_Start') as session_count,\n",
        "        COUNTIF(event = 'Attack_Start') as total_attacks,\n",
        "        \n",
        "        -- Progression metric\n",
        "        MAX(Level) as max_level\n",
        "        \n",
        "    FROM `{TABLE}`\n",
        "    GROUP BY 1\n",
        "),\n",
        "\n",
        "-- Calculate percentile thresholds\n",
        "percentiles AS (\n",
        "    SELECT\n",
        "        -- Top 1% spend threshold (for Whales)\n",
        "        APPROX_QUANTILES(CASE WHEN total_spend > 0 THEN total_spend END, 100)[OFFSET(99)] as whale_threshold,\n",
        "        \n",
        "        -- Top 10% session threshold (for Grinders)\n",
        "        APPROX_QUANTILES(session_count, 100)[OFFSET(90)] as session_90th,\n",
        "        \n",
        "        -- Top 10% attack threshold (for Grinders)\n",
        "        APPROX_QUANTILES(total_attacks, 100)[OFFSET(90)] as attack_90th\n",
        "    FROM user_metrics\n",
        "),\n",
        "\n",
        "-- Classify users into archetypes using EXPLICIT RULES\n",
        "user_summary AS (\n",
        "    SELECT \n",
        "        m.*,\n",
        "        CASE \n",
        "            -- Whales: Top 1% of spenders\n",
        "            WHEN m.total_spend >= p.whale_threshold AND m.total_spend > 0\n",
        "                THEN 'Whale'\n",
        "            \n",
        "            -- Newbies: Never reached Level 2\n",
        "            WHEN m.max_level < 2 \n",
        "                THEN 'Newbie'\n",
        "            \n",
        "            -- Grinders: Non-spenders in top 10% of sessions OR attacks\n",
        "            WHEN m.total_spend = 0 \n",
        "                 AND (m.session_count >= p.session_90th OR m.total_attacks >= p.attack_90th)\n",
        "                THEN 'Grinder'\n",
        "            \n",
        "            -- Casuals: Everyone else (active but moderate engagement)\n",
        "            ELSE 'Casual'\n",
        "        END as player_archetype\n",
        "    FROM user_metrics m\n",
        "    CROSS JOIN percentiles p\n",
        ")\n",
        "\n",
        "SELECT * FROM user_summary\n",
        "\"\"\"\n",
        "\n",
        "df_user_summary = client.query(query_user_summary).to_dataframe()\n",
        "\n",
        "print(\"USER SUMMARY TABLE CREATED\")\n",
        "print(f\"Total users: {len(df_user_summary):,}\")\n",
        "print(f\"\\nArchetype Rules Applied:\")\n",
        "print(\"  â€¢ Whale:   Top 1% of spenders\")\n",
        "print(\"  â€¢ Grinder: $0 spend AND top 10% sessions OR attacks\")\n",
        "print(\"  â€¢ Newbie:  Never reached Level 2\")\n",
        "print(\"  â€¢ Casual:  Everyone else\")\n",
        "print(f\"\\nSample records:\")\n",
        "df_user_summary.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Archetype distribution\n",
        "archetype_summary = df_user_summary.groupby('player_archetype').agg({\n",
        "    'uid': 'count',\n",
        "    'total_spend': ['sum', 'mean'],\n",
        "    'session_count': 'mean',\n",
        "    'total_attacks': 'mean',\n",
        "    'max_level': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "archetype_summary.columns = ['Users', 'Total Revenue', 'Avg Spend', 'Avg Sessions', 'Avg Attacks', 'Avg Level']\n",
        "archetype_summary['% of Users'] = (archetype_summary['Users'] / archetype_summary['Users'].sum() * 100).round(1)\n",
        "archetype_summary['% of Revenue'] = (archetype_summary['Total Revenue'] / archetype_summary['Total Revenue'].sum() * 100).round(1)\n",
        "\n",
        "# Reorder columns\n",
        "archetype_summary = archetype_summary[['Users', '% of Users', 'Total Revenue', '% of Revenue', \n",
        "                                        'Avg Spend', 'Avg Sessions', 'Avg Attacks', 'Avg Level']]\n",
        "\n",
        "# Sort by a logical order\n",
        "archetype_order = ['Whale', 'Grinder', 'Casual', 'Newbie']\n",
        "archetype_summary = archetype_summary.reindex(archetype_order)\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"PLAYER ARCHETYPE SUMMARY\")\n",
        "print(\"=\"*90)\n",
        "print(archetype_summary.to_string())\n",
        "print(\"=\"*90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Statistical Test: Does Archetype Affect Progression?\n",
        "\n",
        "We'll use the **Kruskal-Wallis H-test** (non-parametric ANOVA) to determine if player archetype significantly affects the maximum level reached, followed by **post-hoc pairwise tests with Bonferroni correction**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kruskal-Wallis H-test: Max Level across archetypes\n",
        "# H0: All archetypes have the same distribution of max_level\n",
        "# H1: At least one archetype differs significantly\n",
        "\n",
        "# Prepare data for each archetype\n",
        "whale_levels = df_user_summary[df_user_summary['player_archetype'] == 'Whale']['max_level']\n",
        "grinder_levels = df_user_summary[df_user_summary['player_archetype'] == 'Grinder']['max_level']\n",
        "casual_levels = df_user_summary[df_user_summary['player_archetype'] == 'Casual']['max_level']\n",
        "newbie_levels = df_user_summary[df_user_summary['player_archetype'] == 'Newbie']['max_level']\n",
        "\n",
        "# Perform Kruskal-Wallis H-test\n",
        "h_stat, p_value = kruskal(whale_levels, grinder_levels, casual_levels, newbie_levels)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL TEST: Kruskal-Wallis H-Test\")\n",
        "print(\"Question: Does player archetype affect progression (max level)?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nGroup Statistics (Max Level):\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Calculate effect sizes (median difference from overall median)\n",
        "for name, data in [('Whale', whale_levels), ('Grinder', grinder_levels), \n",
        "                   ('Casual', casual_levels), ('Newbie', newbie_levels)]:\n",
        "    mean_val, ci_low, ci_high = bootstrap_ci(data, statistic=np.mean, n_bootstrap=1000)\n",
        "    print(f\"  {name:8}: Mean={mean_val:.2f} [95% CI: {ci_low:.2f}, {ci_high:.2f}], Median={data.median():.1f}, n={len(data):,}\")\n",
        "\n",
        "print(\"\\nOmnibus Test Results:\")\n",
        "print(\"-\"*60)\n",
        "print(f\"  H-statistic: {h_stat:.2f}\")\n",
        "print(f\"  P-value:     {p_value:.2e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ“ HIGHLY SIGNIFICANT (p < 0.05)\")\n",
        "    print(\"  At least one archetype differs significantly in progression.\")\n",
        "    print(\"  â†’ Proceeding to post-hoc pairwise comparisons...\")\n",
        "else:\n",
        "    print(\"âœ— NOT SIGNIFICANT\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post-hoc pairwise comparisons with Bonferroni correction\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"POST-HOC PAIRWISE COMPARISONS (Mann-Whitney U with Bonferroni Correction)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "archetype_data = {\n",
        "    'Whale': whale_levels,\n",
        "    'Grinder': grinder_levels,\n",
        "    'Casual': casual_levels,\n",
        "    'Newbie': newbie_levels\n",
        "}\n",
        "\n",
        "# All pairwise comparisons\n",
        "comparisons = [\n",
        "    ('Whale', 'Grinder'),\n",
        "    ('Whale', 'Casual'),\n",
        "    ('Whale', 'Newbie'),\n",
        "    ('Grinder', 'Casual'),\n",
        "    ('Grinder', 'Newbie'),\n",
        "    ('Casual', 'Newbie')\n",
        "]\n",
        "\n",
        "# Collect p-values for Bonferroni correction\n",
        "p_values = []\n",
        "results = []\n",
        "\n",
        "for a1, a2 in comparisons:\n",
        "    stat, p = mannwhitneyu(archetype_data[a1], archetype_data[a2], alternative='two-sided')\n",
        "    effect = archetype_data[a1].mean() - archetype_data[a2].mean()\n",
        "    p_values.append(p)\n",
        "    results.append((a1, a2, effect, p))\n",
        "\n",
        "# Bonferroni-adjusted alpha\n",
        "n_tests = len(comparisons)\n",
        "bonferroni_alpha = 0.05 / n_tests\n",
        "\n",
        "print(f\"\\nNumber of comparisons: {n_tests}\")\n",
        "print(f\"Bonferroni-adjusted Î±: {bonferroni_alpha:.4f} (0.05 / {n_tests})\")\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(f\"{'Comparison':<20} {'Î” Mean Level':>12} {'P-value':>12} {'Adj. Sig.':>12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for (a1, a2, effect, p) in results:\n",
        "    sig = \"***\" if p < bonferroni_alpha/10 else \"**\" if p < bonferroni_alpha/2 else \"*\" if p < bonferroni_alpha else \"ns\"\n",
        "    print(f\"{a1:8} vs {a2:8} {effect:>+10.2f} {p:>14.2e} {sig:>10}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(f\"\\nSignificance after Bonferroni correction (Î± = {bonferroni_alpha:.4f}):\")\n",
        "print(f\"  *** p < {bonferroni_alpha/10:.5f}\")\n",
        "print(f\"  **  p < {bonferroni_alpha/2:.5f}\")\n",
        "print(f\"  *   p < {bonferroni_alpha:.5f}\")\n",
        "print(f\"  ns  = not significant after correction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Visualizations: Archetype Profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Radar Chart using Matplotlib (renders on GitHub/Kaggle)\n",
        "# Prepare normalized data\n",
        "radar_metrics = df_user_summary.groupby('player_archetype').agg({\n",
        "    'session_count': 'mean',\n",
        "    'total_attacks': 'mean',\n",
        "    'max_level': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "# Normalize to 0-1 scale using MAX-SCALING\n",
        "radar_normalized = radar_metrics.copy()\n",
        "print(\"NORMALIZATION (Max-Scaling):\")\n",
        "print(\"-\" * 50)\n",
        "for col in radar_normalized.columns:\n",
        "    max_val = radar_normalized[col].max()\n",
        "    print(f\"  {col}: max = {max_val:.1f} â†’ scaled to 1.0\")\n",
        "    if max_val > 0:\n",
        "        radar_normalized[col] = radar_normalized[col] / max_val\n",
        "\n",
        "# Reorder\n",
        "radar_normalized = radar_normalized.reindex(['Whale', 'Grinder', 'Casual', 'Newbie'])\n",
        "\n",
        "print(\"\\nNormalized Values:\")\n",
        "print(radar_normalized.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create radar chart with matplotlib (static - renders on GitHub/Kaggle)\n",
        "categories = ['Sessions', 'Attacks', 'Level']\n",
        "n_cats = len(categories)\n",
        "\n",
        "# Create angles for radar chart\n",
        "angles = [n / float(n_cats) * 2 * np.pi for n in range(n_cats)]\n",
        "angles += angles[:1]  # Close the polygon\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "colors = {'Whale': '#4285F4', 'Grinder': '#FBBC04', 'Casual': '#34A853', 'Newbie': '#EA4335'}\n",
        "\n",
        "for archetype in ['Whale', 'Grinder', 'Casual', 'Newbie']:\n",
        "    values = radar_normalized.loc[archetype].tolist()\n",
        "    values += values[:1]  # Close the polygon\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=archetype, color=colors[archetype])\n",
        "    ax.fill(angles, values, alpha=0.25, color=colors[archetype])\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, size=12)\n",
        "ax.set_ylim(0, 1.1)\n",
        "ax.set_title('Player Archetype Profiles (Normalized to 0-1 Scale)', size=14, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRadar Chart Interpretation:\")\n",
        "print(\"â€¢ Each axis normalized: 1.0 = highest archetype for that metric\")\n",
        "print(\"â€¢ Whales: High across all metrics - engaged AND spending\")\n",
        "print(\"â€¢ Grinders: High engagement (sessions, attacks) but NO spending\")\n",
        "print(\"â€¢ Casuals: Moderate engagement across the board\")\n",
        "print(\"â€¢ Newbies: Low engagement, dropped off early\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box Plot: Revenue distribution by archetype (matplotlib version)\n",
        "df_spenders = df_user_summary[df_user_summary['total_spend'] > 0].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Prepare data for boxplot\n",
        "archetype_order = ['Whale', 'Grinder', 'Casual', 'Newbie']\n",
        "box_data = [df_spenders[df_spenders['player_archetype'] == arch]['total_spend'].values \n",
        "            for arch in archetype_order if len(df_spenders[df_spenders['player_archetype'] == arch]) > 0]\n",
        "box_labels = [arch for arch in archetype_order if len(df_spenders[df_spenders['player_archetype'] == arch]) > 0]\n",
        "\n",
        "bp = ax.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "\n",
        "# Color the boxes\n",
        "colors_list = [colors[label] for label in box_labels]\n",
        "for patch, color in zip(bp['boxes'], colors_list):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "ax.set_xlabel('Archetype')\n",
        "ax.set_ylabel('Total Spend ($)')\n",
        "ax.set_title('Revenue Distribution by Player Archetype (Spenders Only)', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary stats for spenders\n",
        "print(\"\\nSpend Statistics by Archetype (Spenders Only):\")\n",
        "spend_stats = df_user_summary[df_user_summary['total_spend'] > 0].groupby('player_archetype')['total_spend'].describe()\n",
        "print(spend_stats[['count', 'mean', '50%', 'max']].round(2).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pie chart: Archetype composition (matplotlib version)\n",
        "archetype_counts = df_user_summary['player_archetype'].value_counts()\n",
        "archetype_counts = archetype_counts.reindex(['Whale', 'Grinder', 'Casual', 'Newbie'])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "colors_pie = [colors[arch] for arch in archetype_counts.index]\n",
        "wedges, texts, autotexts = ax.pie(archetype_counts.values, \n",
        "                                   labels=archetype_counts.index,\n",
        "                                   colors=colors_pie,\n",
        "                                   autopct='%1.1f%%',\n",
        "                                   pctdistance=0.75,\n",
        "                                   wedgeprops=dict(width=0.5))\n",
        "\n",
        "ax.set_title('Player Base Composition by Archetype', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Style the text\n",
        "for autotext in autotexts:\n",
        "    autotext.set_fontsize(11)\n",
        "    autotext.set_fontweight('bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Strategic Insight: Converting Grinders to Whales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deep dive: Grinder profile with effect sizes\n",
        "grinders = df_user_summary[df_user_summary['player_archetype'] == 'Grinder']\n",
        "whales = df_user_summary[df_user_summary['player_archetype'] == 'Whale']\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GRINDER vs WHALE COMPARISON (with 95% CI)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "metrics = [\n",
        "    ('Sessions', 'session_count'),\n",
        "    ('Attacks', 'total_attacks'),\n",
        "    ('Max Level', 'max_level')\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Metric':<15} {'Grinders':>20} {'Whales':>20} {'Difference':>15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for name, col in metrics:\n",
        "    g_mean, g_ci_lo, g_ci_hi = bootstrap_ci(grinders[col], n_bootstrap=1000)\n",
        "    w_mean, w_ci_lo, w_ci_hi = bootstrap_ci(whales[col], n_bootstrap=1000)\n",
        "    diff = w_mean - g_mean\n",
        "    print(f\"{name:<15} {g_mean:>8.1f} [{g_ci_lo:>5.1f}, {g_ci_hi:>5.1f}] {w_mean:>8.1f} [{w_ci_lo:>5.1f}, {w_ci_hi:>5.1f}] {diff:>+12.1f}\")\n",
        "\n",
        "print(f\"{'Total Spend':<15} {'$0':>20} {'${:.0f}'.format(whales['total_spend'].mean()):>20}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Potential revenue calculation\n",
        "grinder_count = len(grinders)\n",
        "whale_arppu = whales['total_spend'].mean()\n",
        "\n",
        "print(f\"\\nðŸ“ˆ REVENUE OPPORTUNITY: Grinder â†’ Whale Conversion\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Grinder population: {grinder_count:,}\")\n",
        "print(f\"Whale average spend: ${whale_arppu:,.2f}\")\n",
        "print(f\"\\nProjected Revenue by Conversion Rate:\")\n",
        "\n",
        "for rate in [0.05, 0.10, 0.15]:\n",
        "    potential_revenue = grinder_count * rate * whale_arppu\n",
        "    print(f\"  â€¢ {rate*100:.0f}% conversion: ${potential_revenue:,.0f} (+{grinder_count * rate:.0f} new whales)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ Business Strategy: Converting Grinders to Whales\n",
        "\n",
        "Based on this segmentation analysis, here's a data-driven strategy for converting high-engagement non-spenders:\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why Grinders Matter**\n",
        "- **High engagement, zero revenue**: Grinders play as much (or more) than Whales but contribute $0\n",
        "- **Proven retention**: They've already demonstrated long-term commitment to the game\n",
        "- **Lower acquisition cost**: They're already in your funnel â€” no UA spend required\n",
        "\n",
        "#### **Recommended Conversion Tactics**\n",
        "\n",
        "| Tactic | Rationale | Expected Impact |\n",
        "|--------|-----------|----------------|\n",
        "| **1. Progression-Gated Offers** | Grinders care about advancement; offer level-skip packs when they hit walls | Medium-High |\n",
        "| **2. Competitive Advantage Packs** | They're highly active in attacks; offer temporary power-ups for PvP | High |\n",
        "| **3. Time-Saver Bundles** | High session count = time investment; offer resources that reduce grind | Medium |\n",
        "| **4. Exclusive Cosmetics** | Non-P2W items that signal status to other players | Low-Medium |\n",
        "| **5. First-Purchase Discount** | One-time 80% off starter pack to break the $0 barrier | High |\n",
        "\n",
        "#### **Key Principle: Match Offer to Motivation**\n",
        "Grinders are engaged but *haven't found a reason to pay*. Offers must feel like **accelerators**, not requirements.\n",
        "\n",
        "#### **Success Metrics**\n",
        "- Track Grinder â†’ Payer conversion rate (target: 5-10%)\n",
        "- Monitor if converted Grinders reach Whale-level ARPPU\n",
        "- A/B test offer types to find highest-converting bundles\n",
        "\n",
        "---\n",
        "\n",
        "**Bottom Line**: A 10% conversion of Grinders at Whale-level spend would generate significant incremental revenue with minimal acquisition cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Conclusions & Recommendations\n",
        "\n",
        "### Key Findings Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary\n",
        "print(\"=\"*70)\n",
        "print(\"EXECUTIVE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "ðŸ“Š SCALE\n",
        "   â€¢ {df_overview['total_events'].iloc[0]:,} events from {df_overview['unique_users'].iloc[0]:,} users\n",
        "   â€¢ Data period: {df_overview['first_date'].iloc[0]} to {df_overview['last_date'].iloc[0]}\n",
        "\n",
        "ðŸ“ˆ RETENTION\n",
        "   â€¢ Day 1:  ~30-40% (varies by cohort)\n",
        "   â€¢ Day 7:  ~15-20%  \n",
        "   â€¢ Day 30: ~8-12%\n",
        "   â€¢ Statistical test: Cohort differences significant with effect sizes reported\n",
        "\n",
        "ðŸ’° MONETIZATION\n",
        "   â€¢ Conversion Rate: {df_monetization['conversion_rate'].iloc[0]}%\n",
        "   â€¢ ARPPU: ${df_monetization['arppu'].iloc[0]}\n",
        "   â€¢ LTV comparison uses WINDOWED metrics (LTV7, LTV30) to avoid cohort age bias\n",
        "   â€¢ Top 10% of payers drive {top_10_pct:.0f}% of revenue\n",
        "\n",
        "ðŸ‘¥ PLAYER SEGMENTATION (Rule-Based)\n",
        "   â€¢ Whales: Top 1% spenders â€” highest value\n",
        "   â€¢ Grinders: Zero spend, top 10% engagement â€” KEY conversion opportunity\n",
        "   â€¢ Casuals: Moderate engagement â€” bulk of player base\n",
        "   â€¢ Newbies: Early churners â€” FTUE optimization target\n",
        "   â€¢ Kruskal-Wallis + Bonferroni-corrected post-hoc confirms archetype differences\n",
        "\n",
        "âš ï¸ CRITICAL INSIGHTS\n",
        "   â€¢ 81% of first purchases occur at Level 1 â†’ optimize early monetization\n",
        "   â€¢ Level 6 = churn hotspot ({level_6_churn}%) â†’ investigate difficulty/content gap\n",
        "   â€¢ Grinders = untapped revenue â†’ targeted conversion program\n",
        "\"\"\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actionable Recommendations\n",
        "\n",
        "**1. Optimize Early Monetization (High Priority)**\n",
        "- 81% of first purchases occur at Level 1 â†’ maximize early offer exposure\n",
        "- Test starter packs and limited-time offers in first session\n",
        "\n",
        "**2. Address Level 6 Churn Spike (High Priority)**\n",
        "- 23.5% of all churn happens at Level 6\n",
        "- Investigate difficulty spike, content gap, or progression wall\n",
        "\n",
        "**3. Grinder Conversion Program (High Priority)**\n",
        "- Target high-engagement non-spenders with personalized offers\n",
        "- Test progression-gated bundles and competitive advantage packs\n",
        "- Goal: 5-10% conversion rate\n",
        "\n",
        "**4. Improve Day 0 Retention (Medium Priority)**\n",
        "- Optimize first-time user experience (FTUE)\n",
        "- Implement push notification strategy for Day 1 re-engagement\n",
        "\n",
        "**5. Whale Retention Program (Medium Priority)**\n",
        "- Implement VIP program with exclusive content\n",
        "- Monitor whale churn signals for proactive intervention\n",
        "\n",
        "### Methodological Notes\n",
        "\n",
        "- **Windowed LTV**: Used LTV7/LTV30 instead of total LTV to eliminate cohort age bias\n",
        "- **Effect Sizes + CIs**: All statistical tests include practical effect sizes and 95% confidence intervals\n",
        "- **Multiple Comparison Correction**: Post-hoc tests use Bonferroni correction\n",
        "- **Rule-Based Segmentation**: Archetypes defined by explicit business rules (not clustering)\n",
        "\n",
        "### Limitations & Future Work\n",
        "\n",
        "- **Data window**: ~4 months; longer-term LTV projections need more data\n",
        "- **Missing dimensions**: No geographic, platform, or acquisition source data\n",
        "- **Causal inference**: A/B testing needed to validate intervention effectiveness\n",
        "- **Predictive modeling**: Explored but features from first 7 days showed limited discriminative power; sequence-based models or richer features recommended"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Technical Summary\n",
        "\n",
        "**Environment:** Google Cloud Platform (BigQuery + Colab)  \n",
        "**Statistical Methods:**\n",
        "- Chi-square test with effect size (percentage points) and CI\n",
        "- Mann-Whitney U with bootstrap confidence intervals\n",
        "- Kruskal-Wallis H-test with Bonferroni-corrected post-hoc comparisons\n",
        "\n",
        "**Key Methodological Improvements:**\n",
        "- âœ… Windowed LTV (LTV7, LTV30) for fair cohort comparison\n",
        "- âœ… Effect sizes (Î”) in interpretable units alongside p-values\n",
        "- âœ… Bootstrap 95% confidence intervals\n",
        "- âœ… Bonferroni correction for multiple comparisons\n",
        "- âœ… Clear distinction: rule-based segmentation (not clustering)\n",
        "\n",
        "---\n",
        "\n",
        "*Analysis completed using Python 3.10, BigQuery, pandas, scipy, and matplotlib*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
